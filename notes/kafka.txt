Event streaming paltform
-------------------------
- Producers and consumers subscribe to a stream of records
- Store stream of events
- Analyze and process events as they occur

Apache kafka
--------------
Apache kafka is an event streaming platform
- Stores events based on retention time. Events are immutable
- Consumers responsibility to keep track of consumed messages
- Any consumer can access a message from the broker
- It is a distributed streaming system - handles load well

Kafka use cases
----------------
- sending notifications
- alerts
- recommendations
- Tracking

Kafka terminologies and Client APIs
-------------------------------------
- Kafka cluster - consists of multiple kafka brokers
- Zookeeper - Manage multiple brokers, keeps track of the brokers health and manage the cluster
- Kafka clients interact with the brokers
- Kafka clients
    - Kafka Producers - uses the Producer API to write data to kafka
    - Kafka consumers - uses the Consumer API to read data from kafka
    - Both are basic client apis
    - Two other advanced client apis
        - Connect API - Used by Source connector (transfer data from db/file system to kafka cluster) and Sink connector ( transfer data from kafka cluster to db/file syste)
        - Streams API - take data from kafka and perform transformations on it and put it back to kafka

Kafka files
------------
- bin folder - kafka-server-start.sh, zookeeper-server-start.sh, kafka-console-consumer.sh, kafka-console-producer.sh
- for windows there is a windows folder
- config folder
    - server.properties - information about the broker, connection details for the zookeeper

Kafka Topics and Partitions
----------------------------
- Kafka topic - an entity in kafka with a name. Like a table in a db
    - topic resides inside a kafka broker
    - kafka clients use the topic name to produce and consume messages
    - consumer pulls for new messages continuously by polling the topic
    - producer produces a message up on an external trigger
    - message resides in kafka even after being consumed till the retention periods
- Kafka partitions - this is where the messages lives inside a topic
    - topics can have one or more partitions. Mostly there will be multiple partitions
    - partitions have significant effect on scalable message consumption
    - each partition is an ordered, immutable, sequence of records
    - each record is assigned a sequential number called offset (generated when a message is published to the kafka topic)
    - offsets plays an important role in consumers
    - each partition is independent of each other
    - ordering is guaranteed only at the partition level
    - All the records are persisted in a physical log file where kafka is installed
    - Producer has the control to which partition the message goes into

Setting up kafka in local
---------------------------
- First start the zookeeper
- Then start the kafka broker -> the kafka broker registers itself to the zookeeper for it to be managed by the zookeeper
- Zookeeper plays a vital role if there are multiple brokers
- before starting the kafka broker, need to do some configurations in server.properties file like port in which the broker will run, etc.

setting up the kafka -> https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot/blob/master/SetUpKafka.md

Start Zookeeper and Kafka Broker
Start up the Zookeeper.
./zookeeper-server-start.sh ../config/zookeeper.properties

Add the below properties in the server.properties
listeners=PLAINTEXT://localhost:9092
auto.create.topics.enable=false

Start up the Kafka Broker
./kafka-server-start.sh ../config/server.properties


Create topic, produce, consume messages in CLI
------------------------------------------------
- create the topic
    ./kafka-topics.sh --create --topic test-topic --replication-factor 1 --partitions 4 --bootstrap-server localhost:9092
- create the producer (without key)
    ./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
- create the consumer (without key)
    ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning

Sending kafka messages with key and value
- Kafka message has two properties - key and value. Key is optional
- When a message is passed with a key by the producer, the partitioner checks the key and resolves it to a partition and puts the message in that partition.
- When the same key is used, the message is put in the same partition. Thus ensuring order in consuming.
- The consumer polls all the partitions of a topic parallelly. hence ordering is not maintained for messages unless a key is mentioned.

produce a message with key
./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"

consume and print the key
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"


Consumer offsets
------------------
- message will have unique id called offset
- Consumers have three options to read messages - from beginning, latest, specific offset (can be done only programmatically)
- The consumer while reading the message, writes the offset of the read message to an internal topic called __consumer_offsets.
- This acts like a bookmark and when a consumer is crashed and started again, it starts to read the message from where it left off from the partition by using the offset value in the __consumer_offsets topic

- to list all the topics in the broker
./kafka-topics.sh --bootstrap-server localhost:2181 --list

getting the below error
Error while executing topic command : Timed out waiting for a node assignment. Call: listTopics
[2023-03-16 22:59:44,554] ERROR org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: listTopics
 (kafka.admin.TopicCommand$)

 and below exception in zookeeper
[2023-03-16 22:59:07,943] WARN Close of session 0x0 (org.apache.zookeeper.server.NIOServerCnxn)
java.io.IOException: Unreasonable length = 308375649
	at org.apache.jute.BinaryInputArchive.checkLength(BinaryInputArchive.java:166)
	at org.apache.jute.BinaryInputArchive.readBuffer(BinaryInputArchive.java:127)
	at org.apache.zookeeper.proto.ConnectRequest.deserialize(ConnectRequest.java:91)
	at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:1350)
	at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:419)
	at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:180)
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:339)
	at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522)
	at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)



Consumer Groups
----------------
- used for scalable message consumption
- each different application will have a unique consumer group
- kafka broker manages the consumer groups. It acts as the group coordinator
- When creating a consumer instance, a group id is mentioned. Consumer instances with the same group id for a topic will be assigned partitions for polling. if there are 4 partitions and 2 consumers with the same group id, 2 partitions assigned for each one.
- Thus spreading the load.
- To see the consumer groups,
./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

to start a consumer instance with a group id,
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --group <group-name>


Commit log and retention policy
--------------------------------
Commit log
- As soon as a message reaches a topic, it is written to the file system -> log.dirs=/tmp/kafka-logs. folder for each partition will be available
- message is committed after getting logged. The consumer can read only committed messages
- the log is written in byte format. to view the log,
./kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --files /tmp/kafka-logs/test-topic-0/00000000000000000000.log

Retention policy
- how long to retain the message
- policy is configured using the property -> log.retention.hours in server.properties
- default retention period is 168 hours (7 days)

Kafka as a distributed streaming system
----------------------------------------
- Distributed system - multiple systems acts as a single system to the client and delivers a value.
- Characteristics of distributed systems
    - Availability and fault tolerance
    - Reliable work distribution
    - Easily scalable - adding new systems is easy
    - Concurrency handling easily

- In kafka, instead of single broker, multiple brokers can be used.
- This will act as adistributed system and avoid single point of failure
- each broker will send heart beat to the zookeeper
- if a broker is down, the zookeeper knows it and will route the messages to the other working brokers without affecting the clients
- additional brokers can be added with out the clients knowing anything


